# Evaluation Mode Patches for MLOps Overlay
#
# This file patches the base Tekton task to support evaluation mode for individual nodes.
# When "all" is NOT present in EVALUATE_SPECIFIC_NODE, the following steps are skipped:
# - Step 7: upload-to-gdrive (Google Drive upload)
# - Step 8: upload-sarif-to-gcs (GCS upload)
# - Step 9: cleanup
#
# EVALUATE_SPECIFIC_NODE can be:
# - "all" - runs normal SAST workflow + uploads/cleanup
# - "filter" - runs only filter evaluation, skips uploads/cleanup
# - "all,filter" - runs normal SAST workflow + filter evaluation + uploads/cleanup
#
# IMPORTANT: Future upload steps (e.g., S3/MinIO from PR #133) must also include this check:
#   EVAL_NODES="$(params.EVALUATE_SPECIFIC_NODE)"
#   if ! echo ",$EVAL_NODES," | grep -q ",all,"; then
#     echo "Skipping [step name] in evaluation mode"
#     exit 0
#   fi
#
- op: add
  path: /spec/results/-
  value:
    name: evaluation-results
    description: "JSON array of evaluation results from all evaluated nodes"
- op: add
  path: /spec/results/-
  value:
    name: workflow-metrics
    description: "JSON containing workflow metrics (TP/FP/TN/FN, accuracy, precision, recall, F1)"
- op: replace
  path: /spec/steps/5/image
  value: "$(params.CONTAINER_IMAGE)"
- op: replace
  path: /spec/steps/5/script
  value: |
    #!/usr/bin/env sh
    set -ex
    echo "=== STEP 6: RUN SAST AI ANALYSIS OR EVALUATION ==="

    # Load the repo path from step 4
    if [ -f "/shared-data/env.txt" ]; then
      source /shared-data/env.txt
      export REPO_LOCAL_PATH
    else
      echo "Error: No environment file found from prepare-source step" >&2
      exit 1
    fi

    # Use the transformed report file path if available
    if [ -f "/shared-data/report-file-path.txt" ]; then
      TRANSFORMED_REPORT_PATH=$(cat /shared-data/report-file-path.txt)
      export INPUT_REPORT_FILE_PATH="$TRANSFORMED_REPORT_PATH"
      echo "Using transformed report file path: $INPUT_REPORT_FILE_PATH"
    else
      echo "Using original report file path: $INPUT_REPORT_FILE_PATH"
    fi

    # Create directories
    mkdir -p "/cache-data/tmp" "/shared-data/output"

    # Parse and normalize EVALUATE_SPECIFIC_NODE parameter
    EVAL_NODES="$(params.EVALUATE_SPECIFIC_NODE)"
    EVAL_NODES_NORMALIZED=$(echo "$EVAL_NODES" | tr '[:upper:]' '[:lower:]' | sed 's/ //g')
    echo "Evaluation nodes: $EVAL_NODES_NORMALIZED"

    # Check if full workflow should run
    if echo ",$EVAL_NODES_NORMALIZED," | grep -q ",all,"; then
      RUN_FULL_WORKFLOW=true
      echo "Mode: Individual evaluations + full workflow"
    else
      RUN_FULL_WORKFLOW=false
      echo "Mode: Individual evaluations only"
    fi

    # PHASE 1: Individual node evaluations
    mkdir -p /shared-data/eval_results

    # Parse comma-separated list and run each evaluation runner
    echo "$EVAL_NODES_NORMALIZED" | tr ',' '\n' | while read -r node; do
      node=$(echo "$node" | xargs)  # trim whitespace

      # Skip "all" - it's handled by full workflow in Phase 2
      [ "$node" = "all" ] && continue

      # Parse ground truth XLSX file if needed for this evaluation node
      if [ -f "/shared-data/report-file-path.txt" ]; then
        GROUND_TRUTH_PATH=$(cat /shared-data/report-file-path.txt)

        # Check if it's an XLSX file that needs parsing
        if [[ "$GROUND_TRUTH_PATH" =~ \.xlsx$ ]]; then
          mkdir -p /shared-data/evaluation_dataset
          NVR="${PROJECT_NAME}-${PROJECT_VERSION}"

          # Map node names to parser node types
          case "$node" in
            summary)
              PARSER_NODE_TYPE="summarize"
              ;;
            filter|judge)
              PARSER_NODE_TYPE="$node"
              ;;
            *)
              continue
              ;;
          esac

          # Parse XLSX to JSON
          python /app/evaluation/utils/parse_excel_to_json.py \
            --node-type "$PARSER_NODE_TYPE" \
            single \
            --excel-file "$GROUND_TRUTH_PATH" \
            --package-name "$NVR" \
            --output-file "/shared-data/evaluation_dataset/parsed_dataset.json"

          if [ $? -eq 0 ]; then
            export EVALUATION_DATASET_PATH="/shared-data/evaluation_dataset/parsed_dataset.json"
          else
            echo "Error: XLSX parsing failed for $node"
            exit 1
          fi
        fi
      fi

      case "$node" in
        filter)
          echo "Evaluating: filter"
          EVALUATION_JSON_OUTPUT=/shared-data/eval_results/filter.json \
            python /app/evaluation/runners/run_filter_evaluation.py
          ;;
        summary)
          echo "Evaluating: summary"
          EVALUATION_JSON_OUTPUT=/shared-data/eval_results/summary.json \
            python /app/evaluation/runners/run_summarize_evaluation.py
          ;;
        judge)
          echo "Evaluating: judge"
          EVALUATION_JSON_OUTPUT=/shared-data/eval_results/judge.json \
            python /app/evaluation/runners/run_judge_llm_evaluation.py
          ;;
      esac
    done

    # PHASE 2: Run full workflow AFTER evaluations (if "all" was present)
    if [ "$RUN_FULL_WORKFLOW" = "true" ]; then
      echo "Running full SAST workflow..."
      export WORKFLOW_JSON_OUTPUT="/shared-data/output/workflow_metrics.json"
      aiq run --config_file /app/src/sast_agent_workflow/configs/config.yml --input "sast_agent"

      [ -f "/shared-data/output/sast_ai_output.xlsx" ] || { echo "Error: Output file not found!" >&2; exit 1; }

      # Capture workflow metrics
      if [ -f "$WORKFLOW_JSON_OUTPUT" ]; then
        cat "$WORKFLOW_JSON_OUTPUT" > $(results.workflow-metrics.path)
      else
        echo "{}" > $(results.workflow-metrics.path)
      fi
    fi

    # Combine evaluation results
    echo -n "[" > $(results.evaluation-results.path)
    FIRST=true
    for json_file in /shared-data/eval_results/*.json; do
      if [ -f "$json_file" ]; then
        [ "$FIRST" = false ] && echo -n "," >> $(results.evaluation-results.path)
        cat "$json_file" >> $(results.evaluation-results.path)
        FIRST=false
      fi
    done
    echo -n "]" >> $(results.evaluation-results.path)

    echo "Completed successfully"
- op: replace
  path: /spec/steps/7/script
  value: |
    #!/bin/bash
    set -e
    echo "=== STEP 8: UPLOAD SARIF TO GCS BUCKET ==="

    # Skip if "all" is not present in EVALUATE_SPECIFIC_NODE
    EVAL_NODES="$(params.EVALUATE_SPECIFIC_NODE)"
    EVAL_NODES_NORMALIZED=$(echo "$EVAL_NODES" | tr '[:upper:]' '[:lower:]' | sed 's/ //g')
    if ! echo ",$EVAL_NODES_NORMALIZED," | grep -q ",all,"; then
      echo "Skipping GCS upload in evaluation mode (EVALUATE_SPECIFIC_NODE=$EVAL_NODES)"
      exit 0
    fi

    # Check if we have required parameters
    if [ -z "$GCS_BUCKET_NAME" ]; then
      echo "Skipping GCS upload - no bucket name provided"
      exit 0
    fi

    # Check service account
    if [ ! -f "$GOOGLE_APPLICATION_CREDENTIALS" ]; then
      echo "Skipping GCS upload - service account not available"
      exit 0
    fi

    # Look for the SARIF file in output directory
    SARIF_FILE=$(find /shared-data/output -name "*.sarif" -type f 2>/dev/null | head -1)

    if [ -z "$SARIF_FILE" ]; then
      echo "ERROR: No SARIF file found in output directory"
      echo "Available files in output directory:"
      ls -la /shared-data/output/ || echo "Output directory is empty or inaccessible"
      exit 1
    fi

    echo "Found SARIF file: $SARIF_FILE"

    # Install required packages
    echo "Installing required packages..."
    apt-get update -qq >/dev/null 2>&1 && apt-get install -y -qq python3-pip python3-venv >/dev/null 2>&1

    # Create virtual environment
    python3 -m venv /tmp/venv >/dev/null 2>&1
    source /tmp/venv/bin/activate

    # Install Google Cloud Storage
    pip install --quiet google-cloud-storage >/dev/null 2>&1
    echo "Dependencies installed successfully"

    # Generate timestamp for folder organization
    TIMESTAMP=$(date -u +"%Y-%m-%dT%H-%M-%S")

    # Upload the SARIF file
    SCAN_FILENAME=$(basename "$SARIF_FILE")

    # Create organized path: sarif-reports/timestamp/scan_filename
    DESTINATION_PATH="sarif-reports/${TIMESTAMP}/${SCAN_FILENAME}"

    echo "Uploading: $SARIF_FILE"
    echo "Destination: gs://$GCS_BUCKET_NAME/$DESTINATION_PATH"

    if python /scripts/gcs_upload.py "$SARIF_FILE" "$GCS_BUCKET_NAME" "$DESTINATION_PATH"; then
      echo "✓ Successfully uploaded: $SCAN_FILENAME"
      echo "=== SARIF file uploaded to GCS successfully! ==="
    else
      echo "✗ Failed to upload: $SCAN_FILENAME"
      echo "=== SARIF upload failed, but pipeline continues ==="
      # Don't fail the pipeline for upload issues
    fi
- op: replace
  path: /spec/steps/8/script
  value: |
    #!/usr/bin/env sh
    echo "=== STEP 9: CLEANUP ==="

    # Skip if "all" is not present in EVALUATE_SPECIFIC_NODE
    EVAL_NODES="$(params.EVALUATE_SPECIFIC_NODE)"
    EVAL_NODES_NORMALIZED=$(echo "$EVAL_NODES" | tr '[:upper:]' '[:lower:]' | sed 's/ //g')
    if ! echo ",$EVAL_NODES_NORMALIZED," | grep -q ",all,"; then
      echo "Skipping cleanup in evaluation mode (EVALUATE_SPECIFIC_NODE=$EVAL_NODES)"
      exit 0
    fi

    CLEANED_ITEMS=""

    # Remove source code directory
    if [ -d "/shared-data/source" ]; then
        rm -rf /shared-data/source/* >/dev/null 2>&1
        CLEANED_ITEMS="$CLEANED_ITEMS source-code"
    fi

    # Remove false positives file
    if [ -f "/shared-data/false-positives/ignore.err" ]; then
        rm -f /shared-data/false-positives/ignore.err >/dev/null 2>&1
        CLEANED_ITEMS="$CLEANED_ITEMS false-positives"
    fi

    # Clean up any temporary files in cache
    if [ -d "/cache-data/tmp" ]; then
        rm -rf /cache-data/tmp/* >/dev/null 2>&1
        CLEANED_ITEMS="$CLEANED_ITEMS temp-files"
    fi

    # Report what was cleaned
    if [ -n "$CLEANED_ITEMS" ]; then
        echo "Cleaned:$CLEANED_ITEMS"
    else
        echo "Nothing to clean"
    fi

    # Preserve output file
    if [ -f "/shared-data/output/sast_ai_output.xlsx" ]; then
        echo "Output file preserved"
    fi

    echo "Cleanup completed successfully"