"""
Stratified sampling for vulnerability data based on total_defects and perc_known_issues.
"""

import pandas as pd
import numpy as np
import os
from sklearn.model_selection import train_test_split
from typing import Tuple, Literal, Dict, Any
import logging

logger = logging.getLogger(__name__)


def create_defect_severity_groups(df: pd.DataFrame, total_defects_col: str = 'total_defects') -> pd.DataFrame:
    """
    Create severity groups based on total_defects (low, medium, high).
    
    Args:
        df: DataFrame with vulnerability data
        total_defects_col: Name of the total defects column
        
    Returns:
        DataFrame with additional 'defect_severity' column
    """
    df = df.copy()
    
    # Calculate quantile-based thresholds for defect severity
    defects = df[total_defects_col].fillna(0)
    
    # Use tertiles (33rd and 67th percentiles) to create balanced groups
    low_threshold = defects.quantile(0.33)
    high_threshold = defects.quantile(0.67)
    
    def classify_defect_severity(defect_count):
        if defect_count <= low_threshold:
            return 'low'
        elif defect_count <= high_threshold:
            return 'medium'
        else:
            return 'high'
    
    df['defect_severity'] = defects.apply(classify_defect_severity)
    
    logger.info(f"Defect severity thresholds: low ≤ {low_threshold:.1f}, medium ≤ {high_threshold:.1f}, high > {high_threshold:.1f}")
    logger.info(f"Defect severity distribution:\n{df['defect_severity'].value_counts()}")
    
    return df


def create_known_issues_groups(
    df: pd.DataFrame, 
    perc_known_col: str = 'perc_known_issues',
    option: Literal['two_groups', 'three_groups'] = 'two_groups'
) -> pd.DataFrame:
    """
    Create groups based on percentage of known issues.
    
    Args:
        df: DataFrame with vulnerability data
        perc_known_col: Name of the percentage known issues column
        option: 'two_groups' (low/high) or 'three_groups' (low/medium/high)
        
    Returns:
        DataFrame with additional 'known_issues_level' column
    """
    df = df.copy()
    
    known_issues = df[perc_known_col].fillna(0)
    
    if option == 'two_groups':
        # Use median to split into low/high
        threshold = known_issues.median()
        
        def classify_known_issues_2groups(perc):
            return 'low' if perc <= threshold else 'high'
        
        df['known_issues_level'] = known_issues.apply(classify_known_issues_2groups)
        logger.info(f"Known issues threshold (2 groups): low ≤ {threshold:.1f}%, high > {threshold:.1f}%")
        
    else:  # three_groups
        # Use tertiles (33rd and 67th percentiles)
        low_threshold = known_issues.quantile(0.33)
        high_threshold = known_issues.quantile(0.67)
        
        def classify_known_issues_3groups(perc):
            if perc <= low_threshold:
                return 'low'
            elif perc <= high_threshold:
                return 'medium'
            else:
                return 'high'
        
        df['known_issues_level'] = known_issues.apply(classify_known_issues_3groups)
        logger.info(f"Known issues thresholds (3 groups): low ≤ {low_threshold:.1f}%, medium ≤ {high_threshold:.1f}%, high > {high_threshold:.1f}%")
    
    logger.info(f"Known issues distribution:\n{df['known_issues_level'].value_counts()}")
    
    return df


def create_stratification_key(df: pd.DataFrame) -> pd.DataFrame:
    """
    Create composite stratification key from defect_severity and known_issues_level.
    
    Args:
        df: DataFrame with defect_severity and known_issues_level columns
        
    Returns:
        DataFrame with additional 'strata_key' column
    """
    df = df.copy()
    df['strata_key'] = df['defect_severity'] + '_' + df['known_issues_level']
    
    strata_counts = df['strata_key'].value_counts()
    logger.info(f"Stratification groups:\n{strata_counts}")
    
    return df


def vulnerability_stratified_split(
    df: pd.DataFrame,
    total_defects_col: str = 'total_defects',
    perc_known_col: str = 'perc_known_issues',
    known_issues_option: Literal['two_groups', 'three_groups'] = 'two_groups',
    test_size: float = 0.3,
    random_state: int = 42,
    min_samples_per_stratum: int = 2
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    Perform stratified split of vulnerability data.
    
    Args:
        df: DataFrame with vulnerability data
        total_defects_col: Name of the total defects column
        perc_known_col: Name of the percentage known issues column
        known_issues_option: 'two_groups' (6 strata total) or 'three_groups' (9 strata total)
        test_size: Proportion for test set (default: 0.3)
        random_state: Random seed for reproducibility
        min_samples_per_stratum: Minimum samples required per stratum
        
    Returns:
        Tuple of (train_df, test_df)
    """
    logger.info(f"Starting vulnerability stratified split with {known_issues_option}")
    
    # Step 1: Create defect severity groups
    df_with_groups = create_defect_severity_groups(df, total_defects_col)
    
    # Step 2: Create known issues groups
    df_with_groups = create_known_issues_groups(df_with_groups, perc_known_col, known_issues_option)
    
    # Step 3: Create stratification key
    df_with_groups = create_stratification_key(df_with_groups)
    
    # Step 4: Check for small strata
    strata_counts = df_with_groups['strata_key'].value_counts()
    small_strata = strata_counts[strata_counts < min_samples_per_stratum]
    
    if len(small_strata) > 0:
        logger.warning(f"Found {len(small_strata)} strata with < {min_samples_per_stratum} samples:")
        for stratum, count in small_strata.items():
            logger.warning(f"  {stratum}: {count} samples")
        
        # Handle small strata separately
        large_strata_mask = df_with_groups['strata_key'].isin(
            strata_counts[strata_counts >= min_samples_per_stratum].index
        )
        df_large_strata = df_with_groups[large_strata_mask]
        df_small_strata = df_with_groups[~large_strata_mask]
        
        # Stratified split for large strata
        if len(df_large_strata) > 0:
            train_large, test_large = train_test_split(
                df_large_strata,
                test_size=test_size,
                stratify=df_large_strata['strata_key'],
                random_state=random_state
            )
        else:
            train_large = pd.DataFrame()
            test_large = pd.DataFrame()
        
        # Random split for small strata
        if len(df_small_strata) > 0:
            train_small, test_small = train_test_split(
                df_small_strata,
                test_size=test_size,
                random_state=random_state
            )
        else:
            train_small = pd.DataFrame()
            test_small = pd.DataFrame()
        
        # Combine results
        train_df = pd.concat([train_large, train_small], ignore_index=True)
        test_df = pd.concat([test_large, test_small], ignore_index=True)
        
    else:
        # All strata have sufficient samples
        train_df, test_df = train_test_split(
            df_with_groups,
            test_size=test_size,
            stratify=df_with_groups['strata_key'],
            random_state=random_state
        )
    
    # Log results
    logger.info(f"Split completed: {len(train_df)} train, {len(test_df)} test")
    logger.info(f"Train/test ratio: {len(train_df)/len(test_df):.2f}")
    
    # Log distribution balance
    log_split_balance(train_df, test_df, total_defects_col, perc_known_col)
    
    return train_df, test_df


def log_split_balance(
    train_df: pd.DataFrame, 
    test_df: pd.DataFrame,
    total_defects_col: str,
    perc_known_col: str
) -> None:
    """Log the balance of features between train and test sets."""
    
    logger.info("\n" + "="*50)
    logger.info("STRATIFICATION BALANCE REPORT")
    logger.info("="*50)
    
    # Check defect severity balance
    train_defect_dist = train_df['defect_severity'].value_counts(normalize=True).sort_index()
    test_defect_dist = test_df['defect_severity'].value_counts(normalize=True).sort_index()
    
    logger.info("\nDefect Severity Distribution:")
    logger.info(f"{'Severity':<10} {'Train %':<10} {'Test %':<10} {'Diff %':<10}")
    logger.info("-" * 40)
    
    for severity in ['low', 'medium', 'high']:
        train_pct = train_defect_dist.get(severity, 0) * 100
        test_pct = test_defect_dist.get(severity, 0) * 100
        diff = abs(train_pct - test_pct)
        logger.info(f"{severity:<10} {train_pct:<10.1f} {test_pct:<10.1f} {diff:<10.1f}")
    
    # Check known issues balance
    train_known_dist = train_df['known_issues_level'].value_counts(normalize=True).sort_index()
    test_known_dist = test_df['known_issues_level'].value_counts(normalize=True).sort_index()
    
    logger.info("\nKnown Issues Level Distribution:")
    logger.info(f"{'Level':<10} {'Train %':<10} {'Test %':<10} {'Diff %':<10}")
    logger.info("-" * 40)
    
    for level in train_known_dist.index:
        train_pct = train_known_dist.get(level, 0) * 100
        test_pct = test_known_dist.get(level, 0) * 100
        diff = abs(train_pct - test_pct)
        logger.info(f"{level:<10} {train_pct:<10.1f} {test_pct:<10.1f} {diff:<10.1f}")
    
    # Check combined strata balance
    train_strata_dist = train_df['strata_key'].value_counts(normalize=True).sort_index()
    test_strata_dist = test_df['strata_key'].value_counts(normalize=True).sort_index()
    
    logger.info("\nCombined Strata Distribution:")
    logger.info(f"{'Stratum':<20} {'Train %':<10} {'Test %':<10} {'Diff %':<10}")
    logger.info("-" * 50)
    
    all_strata = sorted(set(train_strata_dist.index) | set(test_strata_dist.index))
    for stratum in all_strata:
        train_pct = train_strata_dist.get(stratum, 0) * 100
        test_pct = test_strata_dist.get(stratum, 0) * 100
        diff = abs(train_pct - test_pct)
        logger.info(f"{stratum:<20} {train_pct:<10.1f} {test_pct:<10.1f} {diff:<10.1f}")
    
    # Summary statistics
    logger.info("\nSummary Statistics:")
    logger.info(f"Train set - {total_defects_col}: mean={train_df[total_defects_col].mean():.1f}, std={train_df[total_defects_col].std():.1f}")
    logger.info(f"Test set - {total_defects_col}: mean={test_df[total_defects_col].mean():.1f}, std={test_df[total_defects_col].std():.1f}")
    logger.info(f"Train set - {perc_known_col}: mean={train_df[perc_known_col].mean():.1f}%, std={train_df[perc_known_col].std():.1f}%")
    logger.info(f"Test set - {perc_known_col}: mean={test_df[perc_known_col].mean():.1f}%, std={test_df[perc_known_col].std():.1f}%")


def save_stratified_datasets(
    train_df: pd.DataFrame,
    test_df: pd.DataFrame,
    output_dir: str = "/Users/gziv/Dev/sast-ai-workflow",
    prefix: str = "vulnerability_stratified"
) -> None:
    """
    Save the stratified train and test datasets.
    
    Args:
        train_df: Training dataset
        test_df: Test dataset
        output_dir: Directory to save files
        prefix: Prefix for output filenames
    """
    import os
    
    os.makedirs(output_dir, exist_ok=True)
    
    # Remove helper columns before saving
    helper_cols = ['defect_severity', 'known_issues_level', 'strata_key']
    train_clean = train_df.drop(columns=[col for col in helper_cols if col in train_df.columns])
    test_clean = test_df.drop(columns=[col for col in helper_cols if col in test_df.columns])
    
    train_path = os.path.join(output_dir, f"{prefix}_train.csv")
    test_path = os.path.join(output_dir, f"{prefix}_test.csv")
    
    train_clean.to_csv(train_path, index=False)
    test_clean.to_csv(test_path, index=False)
    
    logger.info(f"Saved train dataset to: {train_path}")
    logger.info(f"Saved test dataset to: {test_path}")


# Example usage function
def check_file_exists_and_warn(file_path: str) -> bool:
    """Check if file exists and warn user. Return True if should continue."""
    if os.path.exists(file_path):
        print(f"⚠️  WARNING: Output file already exists: {file_path}")
        print("❌ STOPPING: Will not overwrite existing files.")
        print("💡 TIP: Move or rename existing files, or change the input filename to generate different output names.")
        return False
    return True


def get_output_prefix(input_path: str) -> str:
    """Generate output prefix from input path with date suffix."""
    from datetime import datetime
    
    # Extract filename without extension
    input_name = os.path.splitext(os.path.basename(input_path))[0]
    
    # Clean up the name (remove spaces, special chars)
    clean_name = input_name.replace(' ', '_').replace('-', '_')
    
    # Add date suffix
    date_suffix = datetime.now().strftime("%d%m%Y")
    
    return f"{clean_name}_stratified_{date_suffix}"


def main():
    """Main function to run vulnerability stratified split."""
    import argparse
    from datetime import datetime
    
    # Setup argument parser
    parser = argparse.ArgumentParser(
        description="Stratified splitting for vulnerability data",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python3 vulnerability_stratified_split.py --input data.xlsx
  python3 vulnerability_stratified_split.py --input data.xlsx --mode 2groups
  python3 vulnerability_stratified_split.py --input data.xlsx --mode 3groups --test-size 0.2
        """
    )
    
    parser.add_argument(
        '--input', 
        type=str, 
        default="INPUT_PATH",
        help='Path to input Excel file (default: INPUT_PATH placeholder)'
    )
    
    parser.add_argument(
        '--mode', 
        choices=['2groups', '3groups', 'both'],
        default='both',
        help='Splitting mode: 2groups (6 strata), 3groups (9 strata), or both (default: both)'
    )
    
    parser.add_argument(
        '--test-size',
        type=float,
        default=0.3,
        help='Test set proportion (default: 0.3)'
    )
    
    parser.add_argument(
        '--random-state',
        type=int,
        default=42,
        help='Random seed for reproducibility (default: 42)'
    )
    
    parser.add_argument(
        '--output-dir',
        type=str,
        default=".",
        help='Output directory (default: current directory)'
    )
    
    args = parser.parse_args()
    
    # Setup logging
    logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
    
    print("="*70)
    print("🎯 VULNERABILITY DATA STRATIFIED SPLITTING")
    print("="*70)
    
    # Check if input path is still placeholder
    if args.input == "INPUT_PATH":
        print("❌ ERROR: Please specify input file path")
        print("💡 USAGE: python3 vulnerability_stratified_split.py --input your_file.xlsx")
        print("📖 For help: python3 vulnerability_stratified_split.py --help")
        return 1
    
    # Check if input file exists
    if not os.path.exists(args.input):
        print(f"❌ ERROR: Input file does not exist: {args.input}")
        print("💡 TIP: Check the file path and make sure the file exists")
        return 1
    
    print(f"📂 Input file: {args.input}")
    print(f"🎛️  Mode: {args.mode}")
    print(f"📊 Test size: {args.test_size:.1%}")
    print(f"🎲 Random state: {args.random_state}")
    print()
    
    try:
        # Load the data
        print("📥 Loading vulnerability data...")
        if args.input.endswith('.xlsx'):
            df = pd.read_excel(args.input)
        elif args.input.endswith('.csv'):
            df = pd.read_csv(args.input)
        else:
            print(f"❌ ERROR: Unsupported file format. Please use .xlsx or .csv files")
            return 1
            
        print(f"✅ Loaded {len(df)} records")
        
        # Check required columns
        required_cols = ['total_defects', 'perc_known_issues']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            print(f"❌ ERROR: Missing required columns: {missing_cols}")
            print(f"📋 Available columns: {list(df.columns)}")
            return 1
        
        # Convert perc_known_issues to percentage if it's in decimal format
        if df['perc_known_issues'].max() <= 1.0:
            df['perc_known_issues'] = df['perc_known_issues'] * 100
            print("🔄 Converted perc_known_issues from decimal to percentage format")
        
        # Remove rows with missing total_defects
        df_clean = df[df['total_defects'].notna()].copy()
        if len(df_clean) < len(df):
            print(f"⚠️  Removed {len(df) - len(df_clean)} rows with missing total_defects")
        
        print(f"📊 Final dataset: {len(df_clean)} records")
        print(f"   total_defects: min={df_clean['total_defects'].min():.1f}, "
              f"max={df_clean['total_defects'].max():.1f}, "
              f"mean={df_clean['total_defects'].mean():.1f}")
        print(f"   perc_known_issues: min={df_clean['perc_known_issues'].min():.1f}%, "
              f"max={df_clean['perc_known_issues'].max():.1f}%, "
              f"mean={df_clean['perc_known_issues'].mean():.1f}%")
        print()
        
        # Generate output prefix
        output_prefix = get_output_prefix(args.input)
        print(f"📁 Output prefix: {output_prefix}")
        
        # Check for existing files before proceeding
        output_files = []
        if args.mode in ['2groups', 'both']:
            output_files.extend([
                os.path.join(args.output_dir, f"{output_prefix}_2groups_train.csv"),
                os.path.join(args.output_dir, f"{output_prefix}_2groups_test.csv")
            ])
        if args.mode in ['3groups', 'both']:
            output_files.extend([
                os.path.join(args.output_dir, f"{output_prefix}_3groups_train.csv"),
                os.path.join(args.output_dir, f"{output_prefix}_3groups_test.csv")
            ])
        
        # Check if any output files already exist
        for output_file in output_files:
            if not check_file_exists_and_warn(output_file):
                return 1
        
        print("✅ All output files are safe to create")
        print()
        
        # Run the selected mode(s)
        if args.mode in ['2groups', 'both']:
            print("🔄 OPTION 1: Two groups for known issues (6 strata)")
            print("="*60)
            
            train_df_2groups, test_df_2groups = vulnerability_stratified_split(
                df_clean,
                total_defects_col='total_defects',
                perc_known_col='perc_known_issues',
                known_issues_option='two_groups',
                test_size=args.test_size,
                random_state=args.random_state
            )
            
            save_stratified_datasets(
                train_df_2groups, 
                test_df_2groups, 
                output_dir=args.output_dir,
                prefix=f"{output_prefix}_2groups"
            )
            print("✅ 2-groups stratification completed!")
            print()
        
        if args.mode in ['3groups', 'both']:
            print("🔄 OPTION 2: Three groups for known issues (9 strata)")
            print("="*60)
            
            train_df_3groups, test_df_3groups = vulnerability_stratified_split(
                df_clean,
                total_defects_col='total_defects',
                perc_known_col='perc_known_issues',
                known_issues_option='three_groups',
                test_size=args.test_size,
                random_state=args.random_state
            )
            
            save_stratified_datasets(
                train_df_3groups, 
                test_df_3groups, 
                output_dir=args.output_dir,
                prefix=f"{output_prefix}_3groups"
            )
            print("✅ 3-groups stratification completed!")
            print()
        
        print("="*70)
        print("🎉 STRATIFIED SPLITTING COMPLETED SUCCESSFULLY!")
        print("="*70)
        print("📁 Generated files:")
        for output_file in output_files:
            if os.path.exists(output_file):
                print(f"   ✅ {os.path.basename(output_file)}")
        print()
        print("🚀 Ready for machine learning model training and evaluation!")
        
        return 0
        
    except Exception as e:
        print(f"❌ ERROR: {str(e)}")
        return 1


if __name__ == "__main__":
    import sys
    import os
    sys.exit(main())