#!/usr/bin/env python3
"""
Stratified Train/Validation/Test Split for Ground-Truth Data with Known Non-Issues

This script splits ground-truth Excel files and paired known non-issue (ignore.err) files
into stratified train, validation, and test sets, ensuring balanced distribution of
false positives/true positives and issue types.

Key Features:
- 60/20/20 split (train/validation/test) with ±3% tolerance
- Maintains FP/TP ratio across all three sets
- Integrates known non-issues (ignore.err files) as 100% FP data
- Package-level pairing: Excel + ignore.err stay together in same split
- Balances both package count AND issue count
- Ensures issue type diversity across all three sets

Usage:
    # Standard run (60/20/20 split)
    python process_mining/scripts/split_train_val_test.py

    # Dry run to preview split
    python process_mining/scripts/split_train_val_test.py --dry-run

    # Custom split ratio
    python process_mining/scripts/split_train_val_test.py --train-ratio 0.70

    # Different random seed
    python process_mining/scripts/split_train_val_test.py --random-seed 123
"""

import sys
import os
from pathlib import Path
from dataclasses import dataclass, field
from typing import List, Dict, Tuple, Optional
import argparse
import logging
import shutil
import random
from collections import defaultdict
import time

sys.path.insert(0, str(Path(__file__).parent.parent.parent))
sys.path.insert(0, str(Path(__file__).parent.parent.parent / 'src'))

from process_mining.src.common.process_mining_config import ProcessMiningConfig
from process_mining.src.common.data_processing_constants import LOG_FORMATS
from process_mining.src.services.process_mining_service import ProcessMiningService
from process_mining.src.services.ignore_err_service import IgnoreErrService
from Utils.file_utils import read_known_errors_file
from Utils.data_access_utils import ExcelFileDiscovery, PackageNameExtractor

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def load_exclusion_list(ground_truth_dir: Path) -> set:
    """
    Load list of files to exclude from invalid_files.txt.

    This file should be generated by preprocess_ground_truth.py and contains
    a list of Excel files that don't have proper annotations.

    Args:
        ground_truth_dir: Path to ground-truth directory

    Returns:
        Set of package names (without .xlsx extension) to exclude
    """
    exclusion_file = ground_truth_dir / 'invalid_files.txt'
    if not exclusion_file.exists():
        logger.info("No invalid_files.txt found. All files will be processed.")
        logger.info("Run preprocess_ground_truth.py first to identify invalid files.")
        return set()

    excluded = set()
    with open(exclusion_file, 'r') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#'):
                excluded.add(line.replace('.xlsx', ''))

    logger.info(f"Loaded {len(excluded)} files to exclude from invalid_files.txt")
    return excluded


@dataclass
class PackageMetadata:
    """Metadata for a single ground-truth package (Excel + optional ignore.err)."""
    nvr: str           
    package_name: str   
    file_path: Path

    excel_issue_count: int
    excel_fp_count: int
    excel_tp_count: int

    ignore_err_path: Optional[Path] = None
    ignore_err_issue_count: int = 0
    ignore_err_issue_types: Dict[str, int] = field(default_factory=dict)

    issue_count: int = 0 
    fp_count: int = 0     
    tp_count: int = 0     
    fp_ratio: float = 0.0

    issue_types: Dict[str, int] = field(default_factory=dict)

    has_ignore_err: bool = False
    pairing_status: str = ""  # "paired", "excel_only"

    excel_entries: List[Dict] = field(default_factory=list) 
    ignore_err_entries: List[Dict] = field(default_factory=list)

    size_category: str = ""
    fp_bucket: str = ""
    stratum: str = ""


class SplitValidator:
    """Validates train/validation/test split quality."""

    def __init__(self, train_packages: List[PackageMetadata], validation_packages: List[PackageMetadata],
                 test_packages: List[PackageMetadata], all_packages: List[PackageMetadata],
                 split_tolerance: float = 0.03, fp_tolerance: float = 0.03):
        self.train_packages = train_packages
        self.validation_packages = validation_packages
        self.test_packages = test_packages
        self.all_packages = all_packages
        self.split_tolerance = split_tolerance
        self.fp_tolerance = fp_tolerance

    def validate_fp_tp_ratio(self) -> Dict:
        """Validate FP/TP ratios in all three sets match overall distribution."""
        overall_total = sum(p.issue_count for p in self.all_packages)
        overall_fp = sum(p.fp_count for p in self.all_packages)
        overall_fp_ratio = overall_fp / overall_total if overall_total > 0 else 0.0

        train_total = sum(p.issue_count for p in self.train_packages)
        train_fp = sum(p.fp_count for p in self.train_packages)
        train_tp = sum(p.tp_count for p in self.train_packages)
        train_fp_ratio = train_fp / train_total if train_total > 0 else 0.0

        val_total = sum(p.issue_count for p in self.validation_packages)
        val_fp = sum(p.fp_count for p in self.validation_packages)
        val_tp = sum(p.tp_count for p in self.validation_packages)
        val_fp_ratio = val_fp / val_total if val_total > 0 else 0.0

        test_total = sum(p.issue_count for p in self.test_packages)
        test_fp = sum(p.fp_count for p in self.test_packages)
        test_tp = sum(p.tp_count for p in self.test_packages)
        test_fp_ratio = test_fp / test_total if test_total > 0 else 0.0

        tolerance = self.fp_tolerance
        train_matches_overall = abs(train_fp_ratio - overall_fp_ratio) <= tolerance
        val_matches_overall = abs(val_fp_ratio - overall_fp_ratio) <= tolerance
        test_matches_overall = abs(test_fp_ratio - overall_fp_ratio) <= tolerance

        return {
            'overall_fp_ratio': overall_fp_ratio,
            'train_total': train_total,
            'train_fp': train_fp,
            'train_tp': train_tp,
            'train_fp_ratio': train_fp_ratio,
            'validation_total': val_total,
            'validation_fp': val_fp,
            'validation_tp': val_tp,
            'validation_fp_ratio': val_fp_ratio,
            'test_total': test_total,
            'test_fp': test_fp,
            'test_tp': test_tp,
            'test_fp_ratio': test_fp_ratio,
            'tolerance': tolerance,
            'is_valid': train_matches_overall and val_matches_overall and test_matches_overall
        }

    def validate_split_ratio(self, total_packages: int, total_issues: int, target_ratio: float = 0.75) -> Dict:
        """Validate that split achieves target ratio for both packages and issues."""
        train_pkg_count = len(self.train_packages)
        test_pkg_count = len(self.test_packages)
        train_pkg_ratio = train_pkg_count / total_packages if total_packages > 0 else 0.0

        train_issue_count = sum(p.issue_count for p in self.train_packages)
        test_issue_count = sum(p.issue_count for p in self.test_packages)
        train_issue_ratio = train_issue_count / total_issues if total_issues > 0 else 0.0

        tolerance = self.split_tolerance
        pkg_valid = abs(train_pkg_ratio - target_ratio) <= tolerance
        issue_valid = abs(train_issue_ratio - target_ratio) <= tolerance

        return {
            'train_pkg_count': train_pkg_count,
            'test_pkg_count': test_pkg_count,
            'train_pkg_ratio': train_pkg_ratio,
            'train_issue_count': train_issue_count,
            'test_issue_count': test_issue_count,
            'train_issue_ratio': train_issue_ratio,
            'target_ratio': target_ratio,
            'tolerance': tolerance,
            'pkg_valid': pkg_valid,
            'issue_valid': issue_valid,
            'is_valid': pkg_valid and issue_valid
        }

    def validate_three_way_split_ratio(self, total_packages: int, total_issues: int) -> Dict:
        """Validate that three-way split achieves target ratios (60/20/20)."""
        train_pkg_count = len(self.train_packages)
        val_pkg_count = len(self.validation_packages)
        test_pkg_count = len(self.test_packages)

        train_pkg_ratio = train_pkg_count / total_packages if total_packages > 0 else 0.0
        val_pkg_ratio = val_pkg_count / total_packages if total_packages > 0 else 0.0
        test_pkg_ratio = test_pkg_count / total_packages if total_packages > 0 else 0.0

        train_issue_count = sum(p.issue_count for p in self.train_packages)
        val_issue_count = sum(p.issue_count for p in self.validation_packages)
        test_issue_count = sum(p.issue_count for p in self.test_packages)

        train_issue_ratio = train_issue_count / total_issues if total_issues > 0 else 0.0
        val_issue_ratio = val_issue_count / total_issues if total_issues > 0 else 0.0
        test_issue_ratio = test_issue_count / total_issues if total_issues > 0 else 0.0

        tolerance = self.split_tolerance

        train_pkg_valid = abs(train_pkg_ratio - 0.60) <= tolerance
        val_pkg_valid = abs(val_pkg_ratio - 0.20) <= tolerance
        test_pkg_valid = abs(test_pkg_ratio - 0.20) <= tolerance

        train_issue_valid = abs(train_issue_ratio - 0.60) <= tolerance
        val_issue_valid = abs(val_issue_ratio - 0.20) <= tolerance
        test_issue_valid = abs(test_issue_ratio - 0.20) <= tolerance

        return {
            'train_pkg_count': train_pkg_count,
            'val_pkg_count': val_pkg_count,
            'test_pkg_count': test_pkg_count,
            'train_pkg_ratio': train_pkg_ratio,
            'val_pkg_ratio': val_pkg_ratio,
            'test_pkg_ratio': test_pkg_ratio,
            'train_issue_count': train_issue_count,
            'val_issue_count': val_issue_count,
            'test_issue_count': test_issue_count,
            'train_issue_ratio': train_issue_ratio,
            'val_issue_ratio': val_issue_ratio,
            'test_issue_ratio': test_issue_ratio,
            'tolerance': tolerance,
            'train_pkg_valid': train_pkg_valid,
            'val_pkg_valid': val_pkg_valid,
            'test_pkg_valid': test_pkg_valid,
            'train_issue_valid': train_issue_valid,
            'val_issue_valid': val_issue_valid,
            'test_issue_valid': test_issue_valid,
            'is_valid': (train_pkg_valid and val_pkg_valid and test_pkg_valid and
                        train_issue_valid and val_issue_valid and test_issue_valid)
        }

    def validate_issue_type_distribution(self, top_n: int = 9) -> Dict:
        """Check that top N issue types appear in all three sets."""
        train_types = defaultdict(int)
        val_types = defaultdict(int)
        test_types = defaultdict(int)
        all_types = defaultdict(int)

        for pkg in self.train_packages:
            for issue_type, count in pkg.issue_types.items():
                train_types[issue_type] += count
                all_types[issue_type] += count

        for pkg in self.validation_packages:
            for issue_type, count in pkg.issue_types.items():
                val_types[issue_type] += count
                all_types[issue_type] += count

        for pkg in self.test_packages:
            for issue_type, count in pkg.issue_types.items():
                test_types[issue_type] += count
                all_types[issue_type] += count

        top_types = sorted(all_types.items(), key=lambda x: x[1], reverse=True)[:top_n]

        distribution = []
        missing_in_train = []
        missing_in_val = []
        missing_in_test = []

        for issue_type, total_count in top_types:
            train_count = train_types.get(issue_type, 0)
            val_count = val_types.get(issue_type, 0)
            test_count = test_types.get(issue_type, 0)

            distribution.append({
                'issue_type': issue_type,
                'total': total_count,
                'train': train_count,
                'validation': val_count,
                'test': test_count,
                'train_pct': train_count / total_count if total_count > 0 else 0.0,
                'val_pct': val_count / total_count if total_count > 0 else 0.0,
                'test_pct': test_count / total_count if total_count > 0 else 0.0
            })

            if train_count == 0:
                missing_in_train.append(issue_type)
            if val_count == 0:
                missing_in_val.append(issue_type)
            if test_count == 0:
                missing_in_test.append(issue_type)

        return {
            'distribution': distribution,
            'missing_in_train': missing_in_train,
            'missing_in_validation': missing_in_val,
            'missing_in_test': missing_in_test,
            'is_valid': len(missing_in_train) == 0 and len(missing_in_val) == 0 and len(missing_in_test) == 0
        }


class StratifiedSplitter:
    """Main class for performing stratified train/validation/test split."""

    def __init__(self, ground_truth_dir: Path, train_ratio: float = 0.60,
                 random_seed: int = 43, config_path: Optional[str] = None,
                 split_tolerance: float = 0.03, fp_tolerance: float = 0.03):
        self.ground_truth_dir = Path(ground_truth_dir)
        self.train_ratio = train_ratio
        self.random_seed = random_seed
        self.config_path = config_path
        self.split_tolerance = split_tolerance
        self.fp_tolerance = fp_tolerance

        self.service = ProcessMiningService(config_path=config_path)
        self.packages: List[PackageMetadata] = []
        self.train_packages: List[PackageMetadata] = []
        self.validation_packages: List[PackageMetadata] = []
        self.test_packages: List[PackageMetadata] = []

    def discover_excel_files(self, exclusion_list: Optional[set] = None) -> List[Path]:
        """
        Discover all .xlsx files in ground-truth/full_dataset directory, excluding invalid ones.

        Args:
            exclusion_list: Set of package names to exclude (without .xlsx extension)

        Returns:
            List of Excel file paths (sorted)
        """
        config = ProcessMiningConfig(self.config_path)
        discovery = ExcelFileDiscovery(config)
        files = discovery.discover_files(
            self.ground_truth_dir,
            use_full_dataset=True,
            exclusion_list=exclusion_list
        )
        return files

    def _extract_package_name(self, excel_path: Path) -> str:
        """
        Extract package name from Excel filename.
        Example: 'ModemManager-1.22.0-3.el10.xlsx' -> 'ModemManager'
        """
        return PackageNameExtractor.from_excel_filename(excel_path.name)

    def _extract_nvr_from_excel(self, excel_path: Path) -> str:
        """
        Extract full NVR from Excel filename.
        Example: 'ModemManager-1.22.0-3.el10.xlsx' -> 'ModemManager-1.22.0-3.el10'
        """
        return excel_path.stem

    def _extract_nvr_from_ignore_err(self, ignore_err_path: Path) -> str:
        """
        Extract full NVR from ignore.err filename.
        Example: 'acl-2.3.2-1.el10_ignore.err' -> 'acl-2.3.2-1.el10'
        """
        filename = ignore_err_path.name
        if filename.endswith('_ignore.err'):
            return filename[:-11]  # Remove '_ignore.err' (11 characters)
        return filename.replace('.err', '')

    def discover_package_pairs(self, exclusion_list: Optional[set] = None) -> Dict[str, Dict]:
        """
        Discover Excel files and their corresponding ignore.err files.

        Returns dict mapping NVR to {
            'excel_path': Path or None,
            'ignore_err_path': Path or None
        }
        """
        pairs = {}

        excel_files = self.discover_excel_files(exclusion_list)
        for excel_path in excel_files:
            nvr = self._extract_nvr_from_excel(excel_path)
            if nvr not in pairs:
                pairs[nvr] = {'excel_path': None, 'ignore_err_path': None}
            pairs[nvr]['excel_path'] = excel_path

        processed_known_non_issues_dir = self.ground_truth_dir / 'processed_known_non_issues'
        if processed_known_non_issues_dir.exists():
            for ignore_err_file in processed_known_non_issues_dir.glob("*_ignore.err"):
                nvr = self._extract_nvr_from_ignore_err(ignore_err_file)

                if nvr not in pairs:
                    pairs[nvr] = {'excel_path': None, 'ignore_err_path': None}
                pairs[nvr]['ignore_err_path'] = ignore_err_file
                logger.debug(f"Discovered NVR-enriched ignore.err: {nvr}")
        else:
            logger.warning(f"Processed known non-issues directory not found: {processed_known_non_issues_dir}")
            logger.warning("Run preprocess_known_non_issues.py first to generate NVR-enriched ignore.err files")

        return pairs

    def parse_ignore_err_file(self, ignore_err_path: Path) -> List[Dict]:
        """
        Parse ignore.err file completely to extract all fields for pattern learning.

        Delegates to IgnoreErrService for centralized parsing logic.

        Returns:
            List of dicts matching Excel format with fields:
            - issue_type, issue_cwe, error_trace, is_false_positive (always True),
              human_justification, comment (None), ai_prediction (None), raw_finding
        """
        service = IgnoreErrService(config_path=self.config_path)
        return service.read_ignore_err_file(ignore_err_path)

    def _assign_size_category(self, issue_count: int) -> str:
        """Assign size category based on issue count using config thresholds."""
        config = ProcessMiningConfig(self.config_path)
        stratification = config.get_stratification_config()
        size_cats = stratification['size_categories']

        if issue_count <= size_cats.get('small_max', 5):
            return 'small'
        elif issue_count <= size_cats.get('medium_max', 20):
            return 'medium'
        else:
            return 'large'

    def _assign_fp_bucket(self, fp_ratio: float) -> str:
        """Assign FP bucket based on FP ratio using config thresholds."""
        config = ProcessMiningConfig(self.config_path)
        stratification = config.get_stratification_config()
        fp_buckets = stratification['fp_ratio_buckets']

        if fp_ratio < fp_buckets.get('low_max', 0.25):
            return 'low'
        elif fp_ratio < fp_buckets.get('high_min', 0.75):
            return 'medium'
        else:
            return 'high'

    def compute_package_metadata_with_pairing(
        self,
        nvr: str,
        package_name: str,
        excel_path: Optional[Path],
        ignore_err_path: Optional[Path],
        pairing_status: str
    ) -> Optional[PackageMetadata]:
        """
        Compute metadata for a package with optional pairing.

        Combines metrics from Excel file and ignore.err file.
        Stores complete entry lists for pattern learning.
        """
        excel_entries = []
        excel_issue_count = 0
        excel_fp_count = 0
        excel_tp_count = 0
        excel_issue_types = {}

        if excel_path:
            try:
                excel_entries = self.service.read_ground_truth_excel(str(excel_path))
                if not excel_entries:
                    logger.warning(f"No entries found in {package_name} Excel, skipping")
                    return None

                excel_issue_count = len(excel_entries)
                excel_fp_count = sum(1 for e in excel_entries if e['is_false_positive'])
                excel_tp_count = excel_issue_count - excel_fp_count

                excel_issue_types = defaultdict(int)
                for entry in excel_entries:
                    excel_issue_types[entry['issue_type']] += 1
                excel_issue_types = dict(excel_issue_types)

            except Exception as e:
                logger.error(f"Failed to process {package_name} Excel: {e}")
                return None

        ignore_err_entries = []
        ignore_err_issue_count = 0
        ignore_err_issue_types = {}

        if ignore_err_path:
            ignore_err_entries = self.parse_ignore_err_file(ignore_err_path)
            ignore_err_issue_count = len(ignore_err_entries)

            ignore_err_issue_types_tmp = defaultdict(int)
            for entry in ignore_err_entries:
                ignore_err_issue_types_tmp[entry['issue_type']] += 1
            ignore_err_issue_types = dict(ignore_err_issue_types_tmp)

        combined_issue_count = excel_issue_count + ignore_err_issue_count
        combined_fp_count = excel_fp_count + ignore_err_issue_count
        combined_tp_count = excel_tp_count
        combined_fp_ratio = combined_fp_count / combined_issue_count if combined_issue_count > 0 else 0.0

        combined_issue_types = defaultdict(int)
        for issue_type, count in excel_issue_types.items():
            combined_issue_types[issue_type] += count
        for issue_type, count in ignore_err_issue_types.items():
            combined_issue_types[issue_type] += count
        combined_issue_types = dict(combined_issue_types)

        size_category = self._assign_size_category(combined_issue_count)
        fp_bucket = self._assign_fp_bucket(combined_fp_ratio)
        stratum = f"{size_category}_{fp_bucket}"

        return PackageMetadata(
            nvr=nvr,
            package_name=package_name,
            file_path=excel_path,
            excel_issue_count=excel_issue_count,
            excel_fp_count=excel_fp_count,
            excel_tp_count=excel_tp_count,
            ignore_err_path=ignore_err_path,
            ignore_err_issue_count=ignore_err_issue_count,
            ignore_err_issue_types=ignore_err_issue_types,
            issue_count=combined_issue_count,
            fp_count=combined_fp_count,
            tp_count=combined_tp_count,
            fp_ratio=combined_fp_ratio,
            issue_types=combined_issue_types,
            has_ignore_err=(ignore_err_path is not None),
            pairing_status=pairing_status,
            excel_entries=excel_entries,
            ignore_err_entries=ignore_err_entries,
            size_category=size_category,
            fp_bucket=fp_bucket,
            stratum=stratum
        )

    def load_and_analyze_packages(self, exclusion_list: Optional[set] = None):
        """
        Load all packages (Excel + ignore.err pairs) and compute metadata.

        Args:
            exclusion_list: Optional set of package names to exclude
        """
        logger.info("Discovering package pairs (Excel + ignore.err)...")
        package_pairs = self.discover_package_pairs(exclusion_list)
        logger.info(f"Found {len(package_pairs)} unique NVRs")

        paired_count = 0
        excel_only_count = 0
        ignore_err_only_count = 0
        skipped_count = 0

        logger.info("Analyzing packages...")
        for nvr, files in package_pairs.items():
            excel_path = files['excel_path']
            ignore_err_path = files['ignore_err_path']

            if excel_path and ignore_err_path:
                pairing_status = "paired"
                paired_count += 1
            elif excel_path:
                pairing_status = "excel_only"
                excel_only_count += 1
            elif ignore_err_path:
                pairing_status = "ignore_err_only"
                ignore_err_only_count += 1
                logger.warning(f"Skipping {nvr}: ignore.err file without matching Excel")
                skipped_count += 1
                continue
            else:
                logger.warning(f"NVR {nvr} has neither Excel nor ignore.err, skipping")
                continue

            package_name = PackageNameExtractor.from_nvr(nvr)

            metadata = self.compute_package_metadata_with_pairing(
                nvr, package_name, excel_path, ignore_err_path, pairing_status
            )

            if metadata:
                self.packages.append(metadata)

        logger.info(f"Successfully analyzed {len(self.packages)} packages")
        logger.info(f"  Paired (Excel + ignore.err): {paired_count}")
        logger.info(f"  Excel only: {excel_only_count}")
        logger.info(f"  Skipped ignore.err-only (no Excel): {skipped_count}")

        total_issues = sum(p.issue_count for p in self.packages)
        total_fp = sum(p.fp_count for p in self.packages)
        total_tp = sum(p.tp_count for p in self.packages)
        excel_issues = sum(p.excel_issue_count for p in self.packages)
        ignore_err_issues = sum(p.ignore_err_issue_count for p in self.packages)

        logger.info(f"Total issues: {total_issues} (Excel: {excel_issues}, ignore.err: {ignore_err_issues})")
        logger.info(f"Total FP: {total_fp}, Total TP: {total_tp}")
        logger.info(f"Overall FP ratio: {total_fp/total_issues:.1%}")

    def group_by_strata(self) -> Dict[str, List[PackageMetadata]]:
        """Group packages into strata."""
        strata = defaultdict(list)
        for pkg in self.packages:
            strata[pkg.stratum].append(pkg)
        return strata

    def _allocate_stratum(self, packages: List[PackageMetadata]) -> Tuple[List[PackageMetadata], List[PackageMetadata]]:
        """
        Allocate packages in one stratum to train/validation/test using greedy dual-objective algorithm.

        Minimizes error from target ratio for BOTH package count and issue count.
        """
        if not packages:
            return [], []

        if len(packages) == 1:
            logger.debug(f"Single package in stratum, assigning to train")
            return packages, []

        packages = sorted(packages, key=lambda p: p.issue_count, reverse=True)
        random.seed(self.random_seed)
        random.shuffle(packages)

        total_packages = len(packages)
        total_issues = sum(p.issue_count for p in packages)

        train_packages = []
        test_packages = []
        train_pkg_count = 0
        train_issue_count = 0

        for pkg in packages:
            new_train_pkg_count = train_pkg_count + 1
            new_train_issue_count = train_issue_count + pkg.issue_count

            pkg_error_if_train = abs(self.train_ratio - new_train_pkg_count / total_packages)
            pkg_error_if_test = abs(self.train_ratio - train_pkg_count / total_packages)

            issue_error_if_train = abs(self.train_ratio - new_train_issue_count / total_issues)
            issue_error_if_test = abs(self.train_ratio - train_issue_count / total_issues)

            combined_error_if_train = pkg_error_if_train + issue_error_if_train
            combined_error_if_test = pkg_error_if_test + issue_error_if_test

            if combined_error_if_train <= combined_error_if_test:
                train_packages.append(pkg)
                train_pkg_count += 1
                train_issue_count += pkg.issue_count
            else:
                test_packages.append(pkg)

        return train_packages, test_packages

    def perform_stratified_split(self):
        """Execute two-stage stratified split: Train/Temp (60/40), then Temp into Validation/Test (50/50)."""
        config = ProcessMiningConfig(self.config_path)
        split_config = config.get_split_config()
        train_pct = int(split_config.get('train_ratio', 0.60) * 100)
        val_pct = int(split_config.get('validation_ratio', 0.20) * 100)
        test_pct = int(split_config.get('test_ratio', 0.20) * 100)

        logger.info(f"Performing two-stage stratified split ({train_pct}/{val_pct}/{test_pct})...")

        temp_pct = 100 - train_pct
        logger.info(LOG_FORMATS['SPLIT_STAGE_1'].format(train=train_pct, temp=temp_pct))
        original_train_ratio = self.train_ratio
        self.train_ratio = 0.60

        strata = self.group_by_strata()
        logger.info(f"Created {len(strata)} strata")

        for stratum_name in sorted(strata.keys()):
            stratum_packages = strata[stratum_name]
            stratum_issues = sum(p.issue_count for p in stratum_packages)
            logger.info(f"  {stratum_name}: {len(stratum_packages)} packages, {stratum_issues} issues")

        self.train_packages = []
        temp_packages = []

        for stratum_name, stratum_packages in strata.items():
            if not stratum_packages:
                continue

            train_stratum, temp_stratum = self._allocate_stratum(stratum_packages)
            self.train_packages.extend(train_stratum)
            temp_packages.extend(temp_stratum)

        logger.info(f"Stage 1 complete: {len(self.train_packages)} train, {len(temp_packages)} temp")

        logger.info(LOG_FORMATS['SPLIT_STAGE_2'].format(val=50, test=50))
        self.train_ratio = 0.50

        temp_strata = defaultdict(list)
        for pkg in temp_packages:
            temp_strata[pkg.stratum].append(pkg)

        self.validation_packages = []
        self.test_packages = []

        for stratum_name, stratum_packages in temp_strata.items():
            if not stratum_packages:
                continue
            val_stratum, test_stratum = self._allocate_stratum(stratum_packages)
            self.validation_packages.extend(val_stratum)
            self.test_packages.extend(test_stratum)

        self.train_ratio = original_train_ratio

        logger.info(f"Stage 2 complete: {len(self.validation_packages)} validation, {len(self.test_packages)} test")
        logger.info(f"Final split: {len(self.train_packages)} train, {len(self.validation_packages)} validation, {len(self.test_packages)} test")

    def validate_split(self) -> Dict:
        """Validate that split meets all requirements."""
        logger.info("Validating split...")

        validator = SplitValidator(
            self.train_packages,
            self.validation_packages,
            self.test_packages,
            self.packages,
            split_tolerance=self.split_tolerance,
            fp_tolerance=self.fp_tolerance
        )

        total_packages = len(self.packages)
        total_issues = sum(p.issue_count for p in self.packages)

        results = {
            'fp_tp_ratio': validator.validate_fp_tp_ratio(),
            'split_ratio': validator.validate_three_way_split_ratio(total_packages, total_issues),
            'issue_type_diversity': validator.validate_issue_type_distribution(),
            'pairing_constraints': self.validate_pairing_constraints(),
            'is_valid': True,
            'errors': []
        }

        if not results['fp_tp_ratio']['is_valid']:
            results['is_valid'] = False
            overall_fp = results['fp_tp_ratio']['overall_fp_ratio']
            train_fp = results['fp_tp_ratio']['train_fp_ratio']
            val_fp = results['fp_tp_ratio']['validation_fp_ratio']
            test_fp = results['fp_tp_ratio']['test_fp_ratio']
            tolerance = results['fp_tp_ratio']['tolerance']
            results['errors'].append(
                f"FP/TP ratio check failed: train={train_fp:.1%}, val={val_fp:.1%}, test={test_fp:.1%} vs overall={overall_fp:.1%} (tolerance: ±{tolerance:.1%})"
            )

        if not results['split_ratio']['is_valid']:
            results['is_valid'] = False
            sr = results['split_ratio']

            if not sr['train_pkg_valid']:
                results['errors'].append(
                    f"Train package ratio check failed: {sr['train_pkg_ratio']:.1%} (target: 60% ± 3%)"
                )
            if not sr['val_pkg_valid']:
                results['errors'].append(
                    f"Validation package ratio check failed: {sr['val_pkg_ratio']:.1%} (target: 20% ± 3%)"
                )
            if not sr['test_pkg_valid']:
                results['errors'].append(
                    f"Test package ratio check failed: {sr['test_pkg_ratio']:.1%} (target: 20% ± 3%)"
                )

            if not sr['train_issue_valid']:
                results['errors'].append(
                    f"Train issue ratio check failed: {sr['train_issue_ratio']:.1%} (target: 60% ± 3%)"
                )
            if not sr['val_issue_valid']:
                results['errors'].append(
                    f"Validation issue ratio check failed: {sr['val_issue_ratio']:.1%} (target: 20% ± 3%)"
                )
            if not sr['test_issue_valid']:
                results['errors'].append(
                    f"Test issue ratio check failed: {sr['test_issue_ratio']:.1%} (target: 20% ± 3%)"
                )

        if not results['issue_type_diversity']['is_valid']:
            results['is_valid'] = False
            missing_train = results['issue_type_diversity']['missing_in_train']
            missing_val = results['issue_type_diversity']['missing_in_validation']
            missing_test = results['issue_type_diversity']['missing_in_test']
            if missing_train:
                results['errors'].append(f"Top issue types missing in train: {', '.join(missing_train)}")
            if missing_val:
                results['errors'].append(f"Top issue types missing in validation: {', '.join(missing_val)}")
            if missing_test:
                results['errors'].append(f"Top issue types missing in test: {', '.join(missing_test)}")

        return results

    def validate_pairing_constraints(self) -> Dict:
        """
        Validate that pairing constraints are maintained.

        Checks that paired packages stay together (Excel + ignore.err in same split).
        """
        errors = []

        paired_packages = [p for p in self.packages if p.has_ignore_err]
        unpaired_packages = [p for p in self.packages if not p.has_ignore_err]

        return {
            'total_packages': len(self.packages),
            'paired_packages': len(paired_packages),
            'unpaired_packages': len(unpaired_packages),
            'errors': errors,
            'is_valid': len(errors) == 0
        }

    def copy_files_to_splits(self):
        """Copy Excel files and ignore.err files to train/, validation/, and test/ subdirectories."""
        logger.info("Copying files to train/, validation/, and test/ directories...")

        train_dir = self.ground_truth_dir / 'train'
        val_dir = self.ground_truth_dir / 'validation'
        test_dir = self.ground_truth_dir / 'test'

        for base_dir in [train_dir, val_dir, test_dir]:
            base_dir.mkdir(exist_ok=True)
            (base_dir / 'excel').mkdir(exist_ok=True)
            (base_dir / 'known_non_issue').mkdir(exist_ok=True)

        for pkg in self.train_packages:
            if pkg.file_path:
                shutil.copy2(pkg.file_path, train_dir / 'excel' / pkg.file_path.name)

            if pkg.ignore_err_path:
                shutil.copy2(pkg.ignore_err_path, train_dir / 'known_non_issue' / pkg.ignore_err_path.name)

        for pkg in self.validation_packages:
            if pkg.file_path:
                shutil.copy2(pkg.file_path, val_dir / 'excel' / pkg.file_path.name)
            if pkg.ignore_err_path:
                shutil.copy2(pkg.ignore_err_path, val_dir / 'known_non_issue' / pkg.ignore_err_path.name)

        for pkg in self.test_packages:
            if pkg.file_path:
                shutil.copy2(pkg.file_path, test_dir / 'excel' / pkg.file_path.name)
            if pkg.ignore_err_path:
                shutil.copy2(pkg.ignore_err_path, test_dir / 'known_non_issue' / pkg.ignore_err_path.name)

        logger.info(f"Copied {len(self.train_packages)} packages to train/")
        logger.info(f"  Excel files: {sum(1 for p in self.train_packages if p.file_path)}")
        logger.info(f"  ignore.err files: {sum(1 for p in self.train_packages if p.ignore_err_path)}")
        logger.info(f"Copied {len(self.validation_packages)} packages to validation/")
        logger.info(f"  Excel files: {sum(1 for p in self.validation_packages if p.file_path)}")
        logger.info(f"  ignore.err files: {sum(1 for p in self.validation_packages if p.ignore_err_path)}")
        logger.info(f"Copied {len(self.test_packages)} packages to test/")
        logger.info(f"  Excel files: {sum(1 for p in self.test_packages if p.file_path)}")
        logger.info(f"  ignore.err files: {sum(1 for p in self.test_packages if p.ignore_err_path)}")

    def generate_manifests(self):
        """Generate manifest files with pairing information."""
        logger.info("Generating manifest files...")

        train_manifest = self.ground_truth_dir / 'train_manifest.txt'
        val_manifest = self.ground_truth_dir / 'validation_manifest.txt'
        test_manifest = self.ground_truth_dir / 'test_manifest.txt'

        def write_manifest(packages, manifest_path):
            package_groups = defaultdict(list)
            for pkg in packages:
                package_groups[pkg.package_name].append(pkg)

            with open(manifest_path, 'w') as f:
                f.write("# Package Manifest - NVR-Based Processing\n")
                f.write("# Format: package_name [paired|excel_only]\n")
                f.write("#\n")
                f.write("# [paired]: Package has both Excel and NVR-enriched ignore.err files\n")
                f.write("# [excel_only]: Package has only Excel file\n\n")

                for package_name in sorted(package_groups.keys()):
                    nvr_list = package_groups[package_name]
                    has_any_ignore_err = any(pkg.has_ignore_err for pkg in nvr_list)
                    pairing_status = "paired" if has_any_ignore_err else "excel_only"
                    f.write(f"{package_name} [{pairing_status}]\n")

        write_manifest(self.train_packages, train_manifest)
        write_manifest(self.validation_packages, val_manifest)
        write_manifest(self.test_packages, test_manifest)

        logger.info(f"Generated {train_manifest}")
        logger.info(f"Generated {val_manifest}")
        logger.info(f"Generated {test_manifest}")

    def generate_split_report(self, validation_results: Dict) -> str:
        """Generate detailed report of split quality."""
        report_lines = []

        report_lines.append("=" * 80)
        report_lines.append("TRAIN/VALIDATION/TEST SPLIT REPORT")
        report_lines.append("=" * 80)
        report_lines.append("")

        total_packages = len(self.packages)
        total_issues = sum(p.issue_count for p in self.packages)
        total_fp = sum(p.fp_count for p in self.packages)
        total_tp = sum(p.tp_count for p in self.packages)

        report_lines.append("## Overall Statistics")
        report_lines.append(f"Total Packages: {total_packages}")
        report_lines.append(f"Total Issues: {total_issues}")
        report_lines.append(f"Overall FP: {total_fp} ({total_fp/total_issues:.1%})")
        report_lines.append(f"Overall TP: {total_tp} ({total_tp/total_issues:.1%})")
        report_lines.append("")

        paired_total = sum(1 for p in self.packages if p.pairing_status == "paired")
        excel_only_total = sum(1 for p in self.packages if p.pairing_status == "excel_only")
        ignore_err_only_total = sum(1 for p in self.packages if p.pairing_status == "ignore_err_only")
        excel_issues_total = sum(p.excel_issue_count for p in self.packages)
        ignore_err_issues_total = sum(p.ignore_err_issue_count for p in self.packages)

        report_lines.append("## Pairing Statistics")
        report_lines.append(f"Paired Packages (Excel + ignore.err): {paired_total} ({paired_total/total_packages:.1%})")
        report_lines.append(f"Excel-Only Packages: {excel_only_total} ({excel_only_total/total_packages:.1%})")
        report_lines.append(f"Ignore.err-Only Packages: {ignore_err_only_total} ({ignore_err_only_total/total_packages:.1%})")
        report_lines.append(f"Total Excel Issues: {excel_issues_total} ({excel_issues_total/total_issues:.1%})")
        report_lines.append(f"Total ignore.err Issues: {ignore_err_issues_total} ({ignore_err_issues_total/total_issues:.1%})")
        report_lines.append("")

        fp_results = validation_results['fp_tp_ratio']
        split_results = validation_results['split_ratio']

        train_paired = sum(1 for p in self.train_packages if p.pairing_status == "paired")
        train_excel_only = sum(1 for p in self.train_packages if p.pairing_status == "excel_only")
        train_ignore_err_only = sum(1 for p in self.train_packages if p.pairing_status == "ignore_err_only")
        train_excel_issues = sum(p.excel_issue_count for p in self.train_packages)
        train_ignore_err_issues = sum(p.ignore_err_issue_count for p in self.train_packages)

        report_lines.append("## Train Split")
        report_lines.append(f"Packages: {split_results['train_pkg_count']} ({split_results['train_pkg_ratio']:.1%})")
        report_lines.append(f"  Paired: {train_paired}, Excel-only: {train_excel_only}, Ignore.err-only: {train_ignore_err_only}")
        report_lines.append(f"Issues: {split_results['train_issue_count']} ({split_results['train_issue_ratio']:.1%})")
        report_lines.append(f"  Excel: {train_excel_issues}, ignore.err: {train_ignore_err_issues}")
        report_lines.append(f"FP: {fp_results['train_fp']} ({fp_results['train_fp_ratio']:.1%})")
        report_lines.append(f"TP: {fp_results['train_tp']} ({1 - fp_results['train_fp_ratio']:.1%})")
        report_lines.append("")

        val_paired = sum(1 for p in self.validation_packages if p.pairing_status == "paired")
        val_excel_only = sum(1 for p in self.validation_packages if p.pairing_status == "excel_only")
        val_ignore_err_only = sum(1 for p in self.validation_packages if p.pairing_status == "ignore_err_only")
        val_excel_issues = sum(p.excel_issue_count for p in self.validation_packages)
        val_ignore_err_issues = sum(p.ignore_err_issue_count for p in self.validation_packages)

        report_lines.append("## Validation Split")
        report_lines.append(f"Packages: {split_results['val_pkg_count']} ({split_results['val_pkg_ratio']:.1%})")
        report_lines.append(f"  Paired: {val_paired}, Excel-only: {val_excel_only}, Ignore.err-only: {val_ignore_err_only}")
        report_lines.append(f"Issues: {split_results['val_issue_count']} ({split_results['val_issue_ratio']:.1%})")
        report_lines.append(f"  Excel: {val_excel_issues}, ignore.err: {val_ignore_err_issues}")
        report_lines.append(f"FP: {fp_results['validation_fp']} ({fp_results['validation_fp_ratio']:.1%})")
        report_lines.append(f"TP: {fp_results['validation_tp']} ({1 - fp_results['validation_fp_ratio']:.1%})")
        report_lines.append("")

        test_paired = sum(1 for p in self.test_packages if p.pairing_status == "paired")
        test_excel_only = sum(1 for p in self.test_packages if p.pairing_status == "excel_only")
        test_ignore_err_only = sum(1 for p in self.test_packages if p.pairing_status == "ignore_err_only")
        test_excel_issues = sum(p.excel_issue_count for p in self.test_packages)
        test_ignore_err_issues = sum(p.ignore_err_issue_count for p in self.test_packages)

        report_lines.append("## Test Split")
        report_lines.append(f"Packages: {split_results['test_pkg_count']} ({split_results['test_pkg_ratio']:.1%})")
        report_lines.append(f"  Paired: {test_paired}, Excel-only: {test_excel_only}, Ignore.err-only: {test_ignore_err_only}")
        report_lines.append(f"Issues: {split_results['test_issue_count']} ({split_results['test_issue_ratio']:.1%})")
        report_lines.append(f"  Excel: {test_excel_issues}, ignore.err: {test_ignore_err_issues}")
        report_lines.append(f"FP: {fp_results['test_fp']} ({fp_results['test_fp_ratio']:.1%})")
        report_lines.append(f"TP: {fp_results['test_tp']} ({1 - fp_results['test_fp_ratio']:.1%})")
        report_lines.append("")

        report_lines.append("## Validation Results")
        tolerance = split_results['tolerance']

        fp_status = "PASS" if fp_results['is_valid'] else "FAIL"
        report_lines.append(f"FP/TP Ratio: [{fp_status}]")
        report_lines.append(f"  Overall FP Ratio: {fp_results['overall_fp_ratio']:.1%}")
        report_lines.append(f"  Train FP Ratio: {fp_results['train_fp_ratio']:.1%} (deviation: {abs(fp_results['train_fp_ratio'] - fp_results['overall_fp_ratio']):.1%})")
        report_lines.append(f"  Validation FP Ratio: {fp_results['validation_fp_ratio']:.1%} (deviation: {abs(fp_results['validation_fp_ratio'] - fp_results['overall_fp_ratio']):.1%})")
        report_lines.append(f"  Test FP Ratio: {fp_results['test_fp_ratio']:.1%} (deviation: {abs(fp_results['test_fp_ratio'] - fp_results['overall_fp_ratio']):.1%})")
        report_lines.append(f"  Tolerance: ±{fp_results['tolerance']:.1%}")
        report_lines.append("")

        split_status = "PASS" if split_results['is_valid'] else "FAIL"
        report_lines.append(f"Split Ratio: [{split_status}]")
        report_lines.append(f"  Train: Packages {split_results['train_pkg_ratio']:.1%}, Issues {split_results['train_issue_ratio']:.1%} (target: 60.0% ±{tolerance:.1%})")
        report_lines.append(f"  Validation: Packages {split_results['val_pkg_ratio']:.1%}, Issues {split_results['val_issue_ratio']:.1%} (target: 20.0% ±{tolerance:.1%})")
        report_lines.append(f"  Test: Packages {split_results['test_pkg_ratio']:.1%}, Issues {split_results['test_issue_ratio']:.1%} (target: 20.0% ±{tolerance:.1%})")
        report_lines.append("")

        diversity_results = validation_results['issue_type_diversity']
        diversity_status = "PASS" if diversity_results['is_valid'] else "FAIL"
        report_lines.append(f"Issue Type Diversity: [{diversity_status}]")
        report_lines.append("")

        report_lines.append("## Top 9 Issue Type Distribution")
        report_lines.append(f"{'Issue Type':<25} {'Total':>8} {'Train':>8} {'Val':>8} {'Test':>8} {'Train %':>9} {'Val %':>9} {'Test %':>9}")
        report_lines.append("-" * 100)

        for item in diversity_results['distribution']:
            report_lines.append(
                f"{item['issue_type']:<25} "
                f"{item['total']:>8} "
                f"{item['train']:>8} "
                f"{item['validation']:>8} "
                f"{item['test']:>8} "
                f"{item['train_pct']:>8.1%} "
                f"{item['val_pct']:>8.1%} "
                f"{item['test_pct']:>8.1%}"
            )
        report_lines.append("")

        report_lines.append("## Stratification Breakdown")
        report_lines.append(f"{'Stratum':<20} {'Train Pkg':>10} {'Val Pkg':>10} {'Test Pkg':>10} {'Train Issues':>13} {'Val Issues':>13} {'Test Issues':>13}")
        report_lines.append("-" * 100)

        strata = self.group_by_strata()
        for stratum_name in sorted(strata.keys()):
            stratum_pkgs = strata[stratum_name]
            train_in_stratum = [p for p in self.train_packages if p.stratum == stratum_name]
            val_in_stratum = [p for p in self.validation_packages if p.stratum == stratum_name]
            test_in_stratum = [p for p in self.test_packages if p.stratum == stratum_name]

            train_pkg_count = len(train_in_stratum)
            val_pkg_count = len(val_in_stratum)
            test_pkg_count = len(test_in_stratum)
            train_issue_count = sum(p.issue_count for p in train_in_stratum)
            val_issue_count = sum(p.issue_count for p in val_in_stratum)
            test_issue_count = sum(p.issue_count for p in test_in_stratum)

            report_lines.append(
                f"{stratum_name:<20} "
                f"{train_pkg_count:>10} "
                f"{val_pkg_count:>10} "
                f"{test_pkg_count:>10} "
                f"{train_issue_count:>13} "
                f"{val_issue_count:>13} "
                f"{test_issue_count:>13}"
            )

        report_lines.append("")

        if validation_results['is_valid']:
            report_lines.append("=" * 80)
            report_lines.append("OVERALL: VALIDATION PASSED")
            report_lines.append("=" * 80)
        else:
            report_lines.append("=" * 80)
            report_lines.append("OVERALL: VALIDATION FAILED")
            report_lines.append("=" * 80)
            report_lines.append("")
            report_lines.append("Errors:")
            for error in validation_results['errors']:
                report_lines.append(f"  - {error}")

        return "\n".join(report_lines)


def main():
    parser = argparse.ArgumentParser(
        description="Split ground-truth data into stratified train/validation/test sets (60/20/20) with known non-issues",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Standard run (60/20/20 split)
  python process_mining/scripts/split_train_val_test.py

  # Dry run to preview split
  python process_mining/scripts/split_train_val_test.py --dry-run

  # Different train ratio (affects three-way split proportions)
  python process_mining/scripts/split_train_val_test.py --train-ratio 0.70

  # Custom random seed
  python process_mining/scripts/split_train_val_test.py --random-seed 123

  # Force proceed even if validation fails
  python process_mining/scripts/split_train_val_test.py --force
        """
    )

    parser.add_argument(
        '--ground-truth-dir',
        type=str,
        help='Path to ground-truth directory (default: from config)'
    )
    parser.add_argument(
        '--train-ratio',
        type=float,
        default=None,
        help='Train split ratio for three-way split (default: from config, 0.60 for 60/20/20 split)'
    )
    parser.add_argument(
        '--random-seed',
        type=int,
        default=None,
        help='Random seed for reproducibility (default: from config, 43)'
    )
    parser.add_argument(
        '--split-tolerance',
        type=float,
        default=None,
        help='Split ratio tolerance for package/issue count validation (default: from config, 0.03 = ±3%%)'
    )
    parser.add_argument(
        '--fp-tolerance',
        type=float,
        default=None,
        help='FP/TP ratio tolerance for distribution matching (default: from config, 0.03 = ±3%%)'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='Show split without copying files'
    )
    parser.add_argument(
        '--force',
        action='store_true',
        help='Proceed even if validation fails'
    )
    parser.add_argument(
        '--config',
        type=str,
        help='Path to process mining config YAML (default: config/process_mining_config.yaml)'
    )
    parser.add_argument(
        '--ignore-exclusions',
        action='store_true',
        help='Ignore invalid_files.txt and process all files'
    )

    args = parser.parse_args()

    config = ProcessMiningConfig(args.config)
    prep_config = config.get_preparation_config()
    split_config = config.get_split_config()

    # Resolve parameters with CLI > config > hardcoded precedence
    train_ratio = args.train_ratio if args.train_ratio is not None else split_config.get('train_ratio', 0.60)
    random_seed = args.random_seed if args.random_seed is not None else split_config.get('random_seed', 43)
    split_tolerance = args.split_tolerance if args.split_tolerance is not None else split_config.get('split_ratio_tolerance', 0.03)
    fp_tolerance = args.fp_tolerance if args.fp_tolerance is not None else split_config.get('fp_tp_ratio_tolerance', 0.03)

    if args.ground_truth_dir:
        ground_truth_dir = Path(args.ground_truth_dir)
    else:
        base_dir = Path(__file__).parent.parent.parent
        ground_truth_dir = base_dir / prep_config.get('ground_truth_dir', 'process_mining/data/ground-truth')

    if not ground_truth_dir.exists():
        logger.error(f"Ground-truth directory not found: {ground_truth_dir}")
        sys.exit(1)

    exclusion_list = set()
    if not args.ignore_exclusions:
        exclusion_list = load_exclusion_list(ground_truth_dir)

    logger.info(f"Ground-truth directory: {ground_truth_dir}")
    logger.info(f"Configuration loaded:")
    logger.info(f"  Train ratio: {train_ratio:.1%}")
    logger.info(f"  Random seed: {random_seed}")
    logger.info(f"  Split ratio tolerance: ±{split_tolerance:.1%}")
    logger.info(f"  FP/TP ratio tolerance: ±{fp_tolerance:.1%}")

    start_time = time.time()
    splitter = StratifiedSplitter(
        ground_truth_dir=ground_truth_dir,
        train_ratio=train_ratio,
        random_seed=random_seed,
        config_path=args.config,
        split_tolerance=split_tolerance,
        fp_tolerance=fp_tolerance
    )

    try:
        splitter.load_and_analyze_packages(exclusion_list)

        if len(splitter.packages) == 0:
            logger.error("No valid packages found!")
            sys.exit(1)

        splitter.perform_stratified_split()

        validation = splitter.validate_split()

        report = splitter.generate_split_report(validation)
        print("\n" + report)

        report_path = ground_truth_dir / 'split_report.txt'
        with open(report_path, 'w') as f:
            f.write(report)
        logger.info(f"\nReport saved to: {report_path}")

        if not validation['is_valid'] and not args.force:
            logger.error("\nValidation failed. Use --force to proceed anyway.")
            sys.exit(1)

        if not args.dry_run:
            splitter.copy_files_to_splits()
            splitter.generate_manifests()

            elapsed = time.time() - start_time
            logger.info(f"\nSplit complete in {elapsed:.1f}s!")
            logger.info(f"Train files: {ground_truth_dir / 'train'}")
            logger.info(f"Validation files: {ground_truth_dir / 'validation'}")
            logger.info(f"Test files: {ground_truth_dir / 'test'}")
        else:
            logger.info("\nDry run complete (no files copied)")

    except KeyboardInterrupt:
        logger.info("\n\nInterrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.exception(f"\n\nFatal error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
