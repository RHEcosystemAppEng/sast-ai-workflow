# NVIDIA Agent Toolkit (NAT) Evaluation for SAST Workflow

This directory contains a complete evaluation system for SAST (Static Application Security Testing) workflows using NVIDIA's Agent Toolkit (NAT) and Phoenix dashboard for observability.

## Overview

The evaluation system tests the quality of security vulnerability analysis summaries generated by AI agents. It uses NVIDIA's LLM-as-judge pattern to score generated summaries against expected outputs across multiple dimensions.

## Files and Components

### Core Scripts

- **`run_full_evaluation.py`** - Main automation script that orchestrates the complete evaluation workflow
- **`eval_config.yml`** - NAT configuration file defining LLMs, evaluation parameters, and scoring weights
- **`summarize_eval_dataset.json`** - Test dataset containing security analysis cases with expected outputs
- **`organize_results.py`** - Script to organize evaluation results into timestamped folders
- **`phoenix_pusher.py`** - Phoenix dashboard integration for visualizing evaluation metrics
- **`requirements.txt`** - Python dependencies for the evaluation environment

### Workflow Integration

- **`src/sast_agent_workflow/tools/summarize_justifications.py`** - Modified SAST workflow tool with NAT-compatible converters

## Evaluation Flow

### 1. Environment Setup
```bash
# Set up virtual environment
python -m venv .venv-test
source .venv-test/bin/activate
pip install -r evaluation/nvidia_eval/requirements.txt

# Set NVIDIA API key
export LLM_API_KEY=your_nvidia_api_key
```

### 2. Run Complete Evaluation
```bash
python evaluation/nvidia_eval/run_full_evaluation.py
```

This executes three sequential steps:

#### Step 1: NAT Evaluation
- Runs NVIDIA Agent Toolkit evaluation using the configured dataset
- Uses NVIDIA LLama-3.1-Nemotron-70B-Instruct as the judge LLM
- Evaluates generated summaries against expected outputs
- Scores on three dimensions: Coverage (50%), Correctness (40%), Relevance (10%)

#### Step 2: Result Organization
- Creates timestamped folders for evaluation results
- Moves output files to organized directory structure
- Creates symlink to latest results for easy access

#### Step 3: Phoenix Dashboard Push
- Creates parent-child span hierarchy using OpenTelemetry
- Pushes evaluation metrics to Phoenix dashboard
- Enables visualization of evaluation results and LLM performance

## Configuration Details

### LLM Configuration (`eval_config.yml`)
- **Evaluation LLM**: NVIDIA Nemotron-70B for generating summaries
- **Judge LLM**: NVIDIA Nemotron-70B for scoring summaries
- **Temperature**: 0.0 for deterministic evaluation
- **Scoring Weights**: Coverage (50%), Correctness (40%), Relevance (10%)

### Evaluation Criteria
The judge LLM evaluates generated summaries based on:

1. **Coverage (50%)**: Does the summary include the same security elements as expected?
   - Same vulnerability type and location
   - Same technical details and mechanisms
   - Same assessment conclusions

2. **Correctness (40%)**: Does the summary match technical details in the expected answer?
   - Same file paths, line numbers, function names
   - Same vulnerability classification
   - Same technical concepts and terminology

3. **Relevance (10%)**: Does the summary have similar style/focus as expected?
   - Similar level of detail
   - Similar professional tone

## Output Files

After evaluation, the following files are generated in timestamped folders:

- **`security_analysis_eval_output.json`** - Detailed evaluation results with scores
- **`workflow_output.json`** - Complete workflow execution data
- **`standardized_data_all.csv`** - Tabular evaluation data
- **`inference_optimization.json`** - LLM performance metrics
- **`all_requests_profiler_traces.json`** - Detailed execution traces

## Phoenix Dashboard

The evaluation results are automatically pushed to Phoenix dashboard for visualization:

- **URL**: http://localhost:6007
- **Project**: sast_eval_agg
- **Features**: Parent-child span hierarchy, evaluation scores, LLM metrics, token usage

## Dataset Structure

The evaluation dataset (`summarize_eval_dataset.json`) contains test cases with:

```json
{
  "id": "test_case_id",
  "question": "test_case_id", 
  "full_justification": "Original detailed security analysis",
  "investigation_result": "TRUE_POSITIVE|FALSE_POSITIVE",
  "issue_type": "SECURITY_ISSUE",
  "severity": "HIGH|MEDIUM|LOW",
  "expected_output_obj": "Expected summary for evaluation"
}
```

## Usage Examples

### Run Complete Workflow
```bash
# Full automated evaluation
export LLM_API_KEY=your_key
python evaluation/nvidia_eval/run_full_evaluation.py
```

### Run Individual Components
```bash
# Just NAT evaluation
source .venv-test/bin/activate
export LLM_API_KEY=your_key
nat eval --config_file evaluation/nvidia_eval/eval_config.yml

# Just organize results
python evaluation/nvidia_eval/organize_results.py

# Just push to Phoenix
python evaluation/nvidia_eval/phoenix_pusher.py
```

## Environment Requirements

- Python 3.8+
- NVIDIA API key for Nemotron models
- Phoenix dashboard running on localhost:6007
- Virtual environment with required dependencies

## Integration Notes

The evaluation system integrates with the main SAST workflow through:

1. **Input Conversion**: Converts evaluation dataset to SASTWorkflowTracker objects
2. **Output Extraction**: Extracts short_justifications from workflow results
3. **Telemetry**: Uses OpenTelemetry for tracing and Phoenix for observability
4. **Backward Compatibility**: Graceful fallback when DeepEval is not available

This setup enables comprehensive evaluation of SAST workflow quality using industry-standard LLM evaluation techniques and observability tools.