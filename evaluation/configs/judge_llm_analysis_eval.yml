# Evaluation Configuration for SAST-AI-Workflow
# Basic token counting and profiling via NAT/AIQ automation

general:
  use_uvloop: true

# Function configuration for judge_llm_analysis
functions:
  judge_llm_analysis:
    _type: judge_llm_analysis
    llm_name: eval_llm

# LLM configuration
llms:
  eval_llm:
    _type: nim
    model_name: nvidia/llama-3.1-nemotron-70b-instruct
    temperature: 0.5
    max_tokens: 1024
    api_key: ${LLM_API_KEY}
    # Force token counting - add specific parameters that might enable usage tracking
    stream_usage: true
    include_usage: true
  judge_llm:
    _type: nim
    model_name: nvidia/llama-3.1-nemotron-70b-instruct
    temperature: 0.5
    max_tokens: 2048
    api_key: ${LLM_API_KEY}

# Workflow configuration
workflow:
  _type: judge_llm_analysis
  llm_name: eval_llm
  input_converter: evaluation.converter_tools.judge_llm_converters.convert_str_to_sast_tracker
  output_converter: evaluation.converter_tools.judge_llm_converters.convert_sast_tracker_to_str

# Evaluation configuration
eval:
  general:
    output_dir: evaluation/reports/judge_llm_analysis

    dataset:
      _type: json
      file_path: evaluation/dataset/judge_llm_eval/judge_llm_eval_dataset_6.json
      structure:
        question_key: "question"
        answer_key: "expected_investigation_result"
        generated_answer_key: "generated_answer"
        trajectory_key: "intermediate_steps"
        expected_trajectory_key: "expected_intermediate_steps"

    # Profiler configuration - automatic token counting and timing via NAT/AIQ
    profiler:
      compute_llm_metrics: true
      csv_exclude_io_text: true
      include_token_usage: true
      track_nvidia_usage: true
      enable_telemetry: true
      bottleneck_analysis:
        enable_nested_stack: false

  # Tunable RAG Evaluator for Justification Quality Assessment
  evaluators:
    justification_quality_eval:
      _type: tunable_rag_evaluator
      llm_name: judge_llm
      # Retry control for rate limiting
      llm_retry_control_params:
        stop_after_attempt: 3
        initial_backoff_delay_seconds: 5
        has_exponential_jitter: true
      # Use custom scoring instead of default
      default_scoring: false
      # Custom prompt for justification quality evaluation
      judge_llm_prompt: >
        You are an expert software security engineer evaluating the quality of justifications for SAST vulnerability analysis.

        **CRITICAL SCORING INSTRUCTIONS - READ CAREFULLY:**

        SEVERELY PENALIZE justifications that exhibit ANY of the following quality issues:
        - **IRRELEVANT CONTENT**: Justifications that discuss topics unrelated to security analysis (travel, cooking, sports, weather, etc.)
        - **CONTRADICTORY ANALYSIS**: When the justification contradicts the provided code or vulnerability description
        - **INCORRECT PREDICTION**: When the investigation_result contradicts the expected result (TRUE POSITIVE vs FALSE POSITIVE mismatch) - this is a MAJOR ISSUE
        - **MISSING SECURITY CONTEXT**: Failing to address the security implications or vulnerability aspects
        - **DAMAGED/INCOMPLETE INFORMATION**: Partial, truncated, or corrupted justification content
        - **NONSENSICAL REASONING**: Logic that doesn't follow or makes no technical sense
        - **OFF-TOPIC EXPLANATIONS**: Any content that strays from security vulnerability analysis

        For justifications with these issues, assign very low scores (0.0-0.3) across ALL dimensions.

        **PREDICTION ACCURACY IS CRITICAL**: If the generated investigation_result (TRUE POSITIVE/FALSE POSITIVE) does not match the expected_investigation_result, this represents a fundamental failure in security analysis and should be heavily penalized across all dimensions, especially TECHNICAL_ACCURACY.

        CRITICAL INSTRUCTION: Evaluate the clarity and completeness of reasoning in the generated justifications.
        Focus on how well the justifications explain the security analysis decision-making process.

        Compare the generated justifications against the expected justifications to assess quality across these dimensions:

        CLARITY (35%): How clear and understandable are the justifications?
        - Uses precise, unambiguous language that explains the reasoning
        - Avoids jargon without explanation or overly complex phrasing
        - Presents information in a logical, easy-to-follow structure
        - Makes the decision-making process transparent and comprehensible
        - Score 1.0 for crystal clear explanations, 0.0 for confusing or unclear reasoning

        COMPLETENESS (30%): How thoroughly does the reasoning cover all relevant aspects?
        - Addresses all key factors that influence the security assessment
        - Explains both positive and negative evidence considered
        - Covers the vulnerability mechanism, impact, and exploitability
        - Discusses relevant code context and execution paths
        - Score 1.0 for comprehensive coverage, 0.0 for superficial or incomplete reasoning

        TECHNICAL ACCURACY (25%): Correctness of security concepts and technical details?
        - Uses correct security terminology and concepts
        - Accurately describes code behavior and potential vulnerabilities
        - Correctly identifies relevant security mechanisms and protections
        - Demonstrates sound understanding of the vulnerability class
        - Score 1.0 for technically precise explanations, 0.0 for incorrect technical details

        LOGICAL FLOW (10%): Coherence and logical progression of the reasoning?
        - Presents arguments in a logical sequence that builds understanding
        - Connects evidence to conclusions in a clear cause-and-effect manner
        - Maintains consistency throughout the justification
        - Flows naturally from problem identification to conclusion
        - Score 1.0 for perfectly logical progression, 0.0 for contradictory or illogical flow

        SCORING REQUIREMENTS:
        - MANDATORY: You MUST calculate the exact final weighted score using this EXACT formula: (clarity_score * 0.35) + (completeness_score * 0.30) + (technical_accuracy_score * 0.25) + (logical_flow_score * 0.10)
        - CRITICAL: Use EXACTLY these decimal weights: 0.35, 0.30, 0.25, 0.10
        - STEP-BY-STEP CALCULATION REQUIRED:
          1. First assign individual scores: CLARITY (0.0-1.0), COMPLETENESS (0.0-1.0), TECHNICAL_ACCURACY (0.0-1.0), LOGICAL_FLOW (0.0-1.0)
          2. Then calculate: (CLARITY × 0.35) + (COMPLETENESS × 0.30) + (TECHNICAL_ACCURACY × 0.25) + (LOGICAL_FLOW × 0.10)
          3. Use the EXACT result of this calculation as your final score - do NOT round or modify it
        - The final score must be a float between 0.0 and 1.0
        - VERIFICATION: Double-check your arithmetic calculation before submitting
        - The reasoning must be 1-2 concise sentences explaining the score based on justification quality and contain the scoring components

        **CRITICAL RETURN FORMAT REQUIREMENTS**:

        1. You MUST return ONLY valid JSON - no additional text before or after
        2. Do NOT use markdown formatting, code blocks, or backticks (```json)
        3. Do NOT include explanatory text outside the JSON
        4. Your response must start with { and end with }
        5. All float values must be between 0.0 and 1
        6. Inside reasoning return the score's components as dict i.e. "CLARITY": clarity_score,
                                                      "COMPLETENESS": completeness_score,
                                                      "TECHNICAL_ACCURACY": technical_accuracy_score,
                                                      "LOGICAL_FLOW": logical_flow_score,
                                                      "Reasoning": "explanation string"
        7. CRITICAL: You MUST verify your final score calculation is exactly: (CLARITY * 0.35) + (COMPLETENESS * 0.30) + (TECHNICAL_ACCURACY * 0.25) + (LOGICAL_FLOW * 0.10)
        8. MANDATORY: The "score" field in your JSON response must be the EXACT mathematical result of the weighted formula calculation - no rounding, no approximation

        IMPORTANT: Your entire response must be valid JSON that can be parsed by json.loads() in Python.

        Focus on the QUALITY of reasoning and explanation, not just correctness of the final decision.
