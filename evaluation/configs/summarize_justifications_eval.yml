# Evaluation Configuration for SAST-AI-Workflow
# Basic token counting and profiling via NAT/AIQ automation

general:
  use_uvloop: true

# Function configuration for summarize_justifications
functions:
  summarize_justifications:
    _type: summarize_justifications
    llm_name: eval_llm

# LLM configuration
llms:
  eval_llm:
    _type: nim
    model_name: nvidia/llama-3.1-nemotron-70b-instruct
    temperature: 0.0
    max_tokens: 1024
    api_key: ${LLM_API_KEY}
    # Force token counting - add specific parameters that might enable usage tracking
    stream_usage: true
    include_usage: true
  judge_llm:
    _type: nim
    model_name: nvidia/llama-3.1-nemotron-70b-instruct
    temperature: 0.0
    max_tokens: 512
    api_key: ${LLM_API_KEY}

# Workflow configuration
workflow:
  _type: summarize_justifications
  llm_name: eval_llm

# Evaluation configuration
eval:
  general:
    output_dir: ./evaluation/reports/summarize_justifications
    dataset:
      _type: json
      file_path: /Users/gziv/Dev/sast-ai-workflow/evaluation/dataset/summarize_eval_dataset.json
      structure:
        question_key: "question"
        answer_key: "expected_output_obj"
        generated_answer_key: "generated_answer"
        trajectory_key: "intermediate_steps"
        expected_trajectory_key: "expected_intermediate_steps"

    # Profiler configuration - automatic token counting and timing via NAT/AIQ
    profiler:
      compute_llm_metrics: true
      csv_exclude_io_text: true
      include_token_usage: true
      track_nvidia_usage: true
      enable_telemetry: true
      bottleneck_analysis:
        enable_nested_stack: false

  # Custom LLM-as-Judge Evaluation Metrics for Summarization
  evaluators:
    summarization_quality_eval:
      _type: tunable_rag_evaluator
      llm_name: judge_llm
      default_scoring: true
      default_score_weights:
        semantic_similarity: 0.35    # How well does the summary capture the meaning
        factual_accuracy: 0.30       # Technical correctness of security details
        conciseness: 0.20            # Brevity and clarity of summary
        professional_tone: 0.15      # Engineering-appropriate language
      judge_llm_prompt: >
        You are an expert software engineer evaluating security vulnerability summary quality.

        CRITICAL INSTRUCTION: Compare the generated summary STRICTLY against the provided expected answer.
        Do NOT evaluate based on your own security knowledge - only compare similarity to the expected answer.

        If the expected answer contains nonsensical content (like "blah blah blah", "cats instead of security",
        empty strings, or irrelevant repetition), the generated summary should receive very low scores
        regardless of its technical quality, because it fails to match the expected nonsensical output.

        If the expected answer contains valid security information, evaluate how well the generated summary matches it:

        SEMANTIC SIMILARITY (35%): Does the generated summary convey the same core security meaning as expected?
        - Same vulnerability type identification (buffer overflow, uninitialized variable, etc.)
        - Same severity assessment conclusion (true positive vs false positive)
        - Same key technical elements mentioned in expected answer
        - Score 1.0 for perfect semantic match, 0.0 for completely different meaning

        FACTUAL ACCURACY (30%): Does the generated summary match the technical facts in the expected answer?
        - Same file paths, line numbers, function names as expected
        - Same variable names and code references as expected
        - Same technical concepts and mechanisms as expected
        - Score 1.0 for perfect factual alignment, 0.0 for contradictory facts

        CONCISENESS (20%): Does the generated summary have similar brevity/detail level as expected?
        - Similar length and information density as expected
        - Avoids unnecessary verbosity compared to expected level
        - Captures essential points without over-elaboration like expected
        - Score 1.0 for optimal length match, 0.0 for severely mismatched verbosity

        PROFESSIONAL TONE (15%): Does the generated summary match the engineering style of expected answer?
        - Similar technical terminology usage as expected
        - Similar formality level as expected
        - Similar structure and presentation as expected
        - Score 1.0 for perfect tone match, 0.0 for completely inappropriate tone

        Return scores 0.0-1.0 for each dimension. Focus on SIMILARITY to expected answer, not absolute quality.